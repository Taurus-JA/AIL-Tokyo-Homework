{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題. Theanoを用いて, CIFAR-10を畳み込みニューラルネットワーク(CNN)で学習せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- homework関数を完成させて提出してください\n",
    "    - 訓練データはtrain_X, train_y, テストデータはtest_Xで与えられます\n",
    "    - train_Xとtrain_yをtrain_X, train_yとvalid_X, valid_yに分けるなどしてモデルを学習させてください\n",
    "    - test_Xに対して予想ラベルpred_yを作り, homework関数の戻り値としてください\\\n",
    "- pred_yのtest_yに対する精度(F値)で評価します\n",
    "- 全体の実行時間がiLect上で60分を超えないようにしてください\n",
    "- homework関数の外には何も書かないでください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のような内容のコードが**事前**に実行されます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from collections import OrderedDict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin-1')\n",
    "    return data\n",
    "\n",
    "trn = [unpickle('/home/ubuntu/cifar_10/data_batch_%d' % i) for i in range(1, 6)]\n",
    "cifar_X_1 = np.concatenate([d['data'] for d in trn]).astype('float32')\n",
    "cifar_y_1 = np.concatenate([d['labels'] for d in trn]).astype('int32')\n",
    "\n",
    "tst = unpickle('/home/ubuntu/cifar_10/test_batch')\n",
    "cifar_X_2 = tst['data'].astype('float32')\n",
    "cifar_y_2 = np.array(tst['labels'], dtype='int32')\n",
    "\n",
    "cifar_X = np.r_[cifar_X_1, cifar_X_2]\n",
    "cifar_y = np.r_[cifar_y_1, cifar_y_2]\n",
    "\n",
    "cifar_X = cifar_X / 255.\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(cifar_X, cifar_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=??)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセルのhomework関数を完成させて提出してください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **上記のコード以外で必要なもの**は全て書いてください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "ilect": {
     "is_homework": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def homework(train_X, train_y, test_X):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    import gc\n",
    "    train_y = np.eye(10)[train_y].astype('int32')\n",
    "    train_X = train_X.reshape((train_X.shape[0], 3, 32, 32))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 3, 32, 32))\n",
    "    train_X = np.r_[train_X,train_X[:, :, :, ::-1]]\n",
    "    train_y = np.r_[train_y, train_y]\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y,\n",
    "                                                          test_size=0.04,\n",
    "                                                          random_state=42)\n",
    "     \n",
    "    def gcn(a):\n",
    "        average = np.mean(a, axis=(1, 2, 3), keepdims=True)\n",
    "        std = np.std(a, axis=(1, 2, 3), keepdims=True)\n",
    "        return (a - average)/std\n",
    "    \n",
    "    class ZCAWhitening:\n",
    "        def __init__(self, epsilon=1e-4):\n",
    "            self.epsilon = epsilon\n",
    "            self.mean = None\n",
    "            self.ZCA_matrix = None\n",
    "\n",
    "        def fit(self, x):\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            #self.mean = np.mean(x, axis=0)\n",
    "            #x -= self.mean\n",
    "            cov_matrix = np.dot(x.T, x) / x.shape[0]\n",
    "            A, d, _ = np.linalg.svd(cov_matrix)\n",
    "            self.ZCA_matrix = np.dot(np.dot(A,\n",
    "                                            np.diag(1. / np.sqrt(d + self.epsilon))\n",
    "                                            ), A.T)\n",
    "\n",
    "        def transform(self, x):\n",
    "            shape = x.shape\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            #x -= self.mean\n",
    "            x = np.dot(x, self.ZCA_matrix.T)\n",
    "            return x.reshape(shape)\n",
    "    \n",
    "    class BatchNorm:\n",
    "    # Constructor\n",
    "        def __init__(self, shape, epsilon=np.float32(1e-5)):\n",
    "            self.shape = shape\n",
    "            self.epsilon = epsilon\n",
    "\n",
    "            self.gamma = theano.shared(np.ones(self.shape, dtype=\"float32\"),\n",
    "                                       name=\"gamma\")\n",
    "            self.beta = theano.shared(np.zeros(self.shape, dtype=\"float32\"),\n",
    "                                      name=\"beta\")\n",
    "            self.params = [self.gamma, self.beta]\n",
    "\n",
    "    # Forward Propagation\n",
    "        def f_prop(self, x):\n",
    "            if x.ndim == 2:\n",
    "                mean = T.mean(x, axis=0, keepdims=True)\n",
    "                std = T.sqrt(T.var(x, axis=0, keepdims=True) + self.epsilon)\n",
    "            elif x.ndim == 4:\n",
    "                mean = T.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "                std = T.sqrt(T.var(x, axis=(0, 2, 3), keepdims=True) +\n",
    "                             self.epsilon)\n",
    "\n",
    "            normalized_x = (x - mean) / std\n",
    "            self.z = self.gamma * normalized_x + self.beta\n",
    "            return self.z\n",
    "        \n",
    "    class Conv:\n",
    "    # Constructor\n",
    "        def __init__(self, filter_shape, function=lambda x: x, border_mode=\"valid\",\n",
    "                     subsample=(1, 1)):\n",
    "            self.function = function\n",
    "            self.border_mode = border_mode\n",
    "            self.subsample = subsample\n",
    "\n",
    "            fan_in = np.prod(filter_shape[1:])\n",
    "            fan_out = (filter_shape[0] * np.prod(filter_shape[2:]))\n",
    "            self.W = theano.shared(np.random.uniform(\n",
    "                        low=-np.sqrt(6. / (fan_in + fan_out)),\n",
    "                        high=np.sqrt(6. / (fan_in + fan_out)),\n",
    "                        size=filter_shape\n",
    "                    ).astype(\"float32\"), name=\"W\")\n",
    "            self.b = theano.shared(np.zeros((filter_shape[0],), dtype=\"float32\"),\n",
    "                                   name=\"b\")\n",
    "            self.params = [self.W, self.b]\n",
    "\n",
    "    # Forward Propagation\n",
    "        def f_prop(self, x):\n",
    "            conv_out = conv2d(x, self.W, border_mode=self.border_mode,\n",
    "                              subsample=self.subsample)\n",
    "            self.z = self.function(conv_out +\n",
    "                                   self.b[np.newaxis, :, np.newaxis, np.newaxis])\n",
    "            return self.z\n",
    "    \n",
    "    class Pooling:\n",
    "    # Constructor\n",
    "        def __init__(self, pool_size=(2, 2), padding=(0, 0), mode='max'):\n",
    "            self.pool_size = pool_size\n",
    "            self.mode = mode\n",
    "            self.padding = padding\n",
    "            self.params = []\n",
    "\n",
    "    # Forward Propagation\n",
    "        def f_prop(self, x):\n",
    "            return pool.pool_2d(input=x, ds=self.pool_size, padding=self.padding,\n",
    "                                mode=self.mode, ignore_border=True)\n",
    "        \n",
    "    class Flatten:\n",
    "    # Constructor\n",
    "        def __init__(self, outdim=2):\n",
    "            self.outdim = outdim\n",
    "            self.params = []\n",
    "\n",
    "    # Forward Propagation\n",
    "        def f_prop(self, x):\n",
    "            return T.flatten(x, self.outdim)\n",
    "        \n",
    "    class Layer:\n",
    "    # Constructor\n",
    "        def __init__(self, in_dim, out_dim, function, possibility):\n",
    "            self.in_dim = in_dim\n",
    "            self.out_dim = out_dim\n",
    "            self.function = function\n",
    "            self.possibility = possibility\n",
    "            self.W = theano.shared(np.random.uniform(\n",
    "                        low=-np.sqrt(6. / (in_dim + out_dim)),\n",
    "                        high=np.sqrt(6. / (in_dim + out_dim)),\n",
    "                        size=(in_dim, out_dim)\n",
    "                    ).astype(\"float32\"), name=\"W\")\n",
    "\n",
    "            self.b = theano.shared(np.zeros(out_dim).astype(\"float32\"), name=\"b\")\n",
    "            self.params = [self.W, self.b]\n",
    "\n",
    "    # Forward Propagation\n",
    "        def f_prop(self, x):\n",
    "            self.z = self.function(T.dot(x, self.W) + self.b)\n",
    "            return self.z\n",
    "        \n",
    "        def get_mask(self):\n",
    "            a = np.random.rand(self.out_dim) < self.possibility\n",
    "            return a*np.float32(1.0)\n",
    "        \n",
    "    class Activation:\n",
    "    # Constructor\n",
    "        def __init__(self, function):\n",
    "            self.function = function\n",
    "            self.params = []\n",
    "\n",
    "    # Forward Propagation\n",
    "        def f_prop(self, x):\n",
    "            self.z = self.function(x)\n",
    "            return self.z\n",
    "    \n",
    "    activation = T.nnet.relu\n",
    "    \n",
    "    def build_shared_zeros(shape, name):\n",
    "\n",
    "        return theano.shared(value=np.zeros(shape, dtype=theano.config.floatX), \n",
    "                             name=name, borrow=True)\n",
    "    \n",
    "    class Adam:\n",
    "        def __init__(self, params, alpha=0.0005, beta1=0.9, beta2=0.999, eps=1e-8, gamma=1-1e-8):\n",
    "            self.alpha = alpha\n",
    "            self.b1 = beta1\n",
    "            self.b2 = beta2\n",
    "            self.gamma = gamma\n",
    "            self.t = theano.shared(np.float32(1))\n",
    "            self.eps = eps\n",
    "\n",
    "            self.ms = [build_shared_zeros(t.shape.eval(), 'm') for t in params]\n",
    "            self.vs = [build_shared_zeros(t.shape.eval(), 'v') for t in params]\n",
    "        \n",
    "        def updates(self, g_params, cost):\n",
    "            self.b1_t = self.b1 * self.gamma ** (self.t - 1)\n",
    "            self.updates = OrderedDict()\n",
    "            for m, v, param, g_param in zip(self.ms, self.vs, params, g_params):\n",
    "                _m = self.b1_t * m + (1 - self.b1_t) * g_param\n",
    "                _v = self.b2 * v + (1 - self.b2) * g_param ** 2\n",
    "\n",
    "                m_hat = _m / (1 - self.b1 ** self.t)\n",
    "                v_hat = _v / (1 - self.b2 ** self.t)\n",
    "\n",
    "                self.updates[param] = param - self.alpha*m_hat / (T.sqrt(v_hat) + self.eps)\n",
    "                self.updates[m] = _m\n",
    "                self.updates[v] = _v\n",
    "            self.updates[self.t] = self.t + 1.0\n",
    "\n",
    "            return self.updates\n",
    "        \n",
    "    layers = [                                                    # (チャネル数)x(縦の次元数)x(横の次元数)\n",
    "        Conv((32, 3, 3, 3), border_mode = (1,1)),                 # 3x32x32  ->  32x32x32 No.0\n",
    "        BatchNorm((32, 32, 32)),\n",
    "        Activation(activation),\n",
    "        Conv((64, 32, 3, 3), border_mode = (1,1)),                # 32x32x32 ->  64x32x32 No.3\n",
    "        BatchNorm((64, 32, 32)),\n",
    "        Activation(activation),\n",
    "        Pooling((2, 2)),                                          # 64x32x32 ->  64x16x16 No.6\n",
    "        BatchNorm((64, 16, 16)),\n",
    "        Conv((128, 64, 3, 3), border_mode = (1,1)),               # 64x16x16   -> 128x16x16　No.8\n",
    "        BatchNorm((128,16,16)),\n",
    "        Activation(activation),\n",
    "        Conv((128,128, 3, 3), border_mode = (1,1)),               # 128x16x16 -> 128x16x16 No.11\n",
    "        BatchNorm((128,16,16)),\n",
    "        Activation(activation),\n",
    "        Conv((128,128, 3, 3), border_mode = (1,1)),               # 128x16x16 -> 128x16x16 No.14\n",
    "        BatchNorm((128,16,16)),\n",
    "        Activation(activation),\n",
    "        Pooling((2, 2)),                                          # 128x16x16 -> 128x8x8 No.17\n",
    "        BatchNorm((128, 8, 8)),\n",
    "        Activation(activation),\n",
    "        Conv((128, 128, 3, 3), border_mode = (1,1)),              # 128x8x8  ->  128x8x8 No.20\n",
    "        BatchNorm((128, 8, 8)),\n",
    "        Activation(activation),\n",
    "        Conv((128, 128, 3, 3), border_mode = (1,1)),              # 128x8x8  ->  128x8x8 No.23\n",
    "        BatchNorm((128, 8, 8)),\n",
    "        Activation(activation),\n",
    "        Conv((128, 128, 3, 3), border_mode = (1,1)),              # 128x8x8 ->  128x8x8 No.26\n",
    "        BatchNorm((128, 8, 8)),\n",
    "        Activation(activation),\n",
    "        Pooling((2, 2)),                                          # 128x8x8  -> 128x4x4 No.29\n",
    "        BatchNorm((128, 4, 4)),\n",
    "        Activation(activation),\n",
    "        Conv((128, 128, 3, 3), border_mode = (1,1)),              # 128x4x4  ->  128x4x4 No.32\n",
    "        BatchNorm((128, 4, 4)),\n",
    "        Activation(activation),\n",
    "        Conv((128, 128, 3, 3), border_mode = (1,1)),              # 128x4x4 ->  128x4x4 No.35\n",
    "        BatchNorm((128, 4, 4)),\n",
    "        Activation(activation),\n",
    "        Pooling((2, 2)),                                          # 128x4x4  -> 128x2x2 No.38\n",
    "        Flatten(2),\n",
    "        Layer(128*2*2, 256, activation, 0.75),\n",
    "        Layer(256, 10, T.nnet.softmax, 1.00)\n",
    "    ]\n",
    "    \n",
    "    x = T.ftensor4('x')\n",
    "    t = T.imatrix('t')\n",
    "\n",
    "    params = []\n",
    "    layer_out = x\n",
    "    for (i, layer) in enumerate(layers):\n",
    "        params += layer.params\n",
    "        if i < 40:\n",
    "            layer_out = layer.f_prop(layer_out)\n",
    "        else:\n",
    "            layer.mask = layer.get_mask()\n",
    "            layer_out = layer.f_prop(layer_out)*layer.mask\n",
    "    y = layers[-1].z\n",
    "\n",
    "    cost = T.mean(T.nnet.categorical_crossentropy(y, t))\n",
    "\n",
    "    g_params = T.grad(cost, params)\n",
    "    updates = Adam(params).updates(g_params, cost)\n",
    "    \n",
    "    train = theano.function(inputs=[x, t], outputs=cost, updates=updates,\n",
    "                            allow_input_downcast=True, name='train')\n",
    "    valid = theano.function(inputs=[x, t], outputs=[cost, T.argmax(y, axis=1)],\n",
    "                            allow_input_downcast=True, name='valid')\n",
    "    test = theano.function(inputs=[x], outputs=T.argmax(y, axis=1), allow_input_downcast=True,\n",
    "                           name='test')\n",
    "    \n",
    "    zca = ZCAWhitening()\n",
    "    zca.fit(gcn(train_X))\n",
    "    zca_train_X = zca.transform(gcn(train_X))\n",
    "    zca_train_y = train_y[:]\n",
    "    zca_valid_X = zca.transform(gcn(valid_X))\n",
    "    zca_valid_y = valid_y[:]\n",
    "    zca.fit(gcn(test_X))\n",
    "    zca_test_X = zca.transform(gcn(test_X))\n",
    "    \n",
    "    del train_X, train_y\n",
    "    \n",
    "    batch_size = 100\n",
    "    n_batches = zca_train_X.shape[0]//batch_size\n",
    "    epoch = 1\n",
    "    while time.time() - start_time < 45*60 and epoch < 20:\n",
    "        zca_train_X, zca_train_y = shuffle(zca_train_X, zca_train_y)\n",
    "        for i in range(n_batches):\n",
    "            start = i*batch_size\n",
    "            end = start + batch_size\n",
    "            cost = train(zca_train_X[start:end], zca_train_y[start:end])\n",
    "        print('Training cost: %.3f' % cost)\n",
    "        valid_cost, pred_y = valid(zca_valid_X, zca_valid_y)\n",
    "        print('EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f' %\n",
    "                (epoch , valid_cost,\n",
    "                f1_score(np.argmax(zca_valid_y, axis=1).astype('int32'),\n",
    "                        pred_y, average='macro')))\n",
    "        \n",
    "        print('Running time is %i minutes' % int((time.time()- start_time)/60))\n",
    "        epoch += 1\n",
    "        \n",
    "    del zca, zca_valid_X, zca_train_X, zca_valid_y, zca_train_y\n",
    "    gc.collect()\n",
    "    batch_size = 100\n",
    "    n_batches = test_X.shape[0]//batch_size\n",
    "    pred_y = []\n",
    "    for i in range(n_batches):\n",
    "        start = i*batch_size\n",
    "        end = start + batch_size\n",
    "        pred_y = np.r_[pred_y, test(zca_test_X[start:end])]\n",
    "        \n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "ilect": {
     "course_id": 4,
     "course_rank": 8,
     "is_evaluation": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device gpu failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cuda error: kernel_reduce_ccontig_node_meb404c8cd39208f6884dd773b584b7d7_0: out of memory. (grid: 1 x 1; block: 256 x 1 x 1)\n\nApply node that caused the error: GpuCAReduce{add}{1}(<CudaNdarrayType(float32, vector)>)\nToposort index: 0\nInputs types: [CudaNdarrayType(float32, vector)]\nInputs shapes: [(10000,)]\nInputs strides: [(1,)]\nInputs values: ['not shown']\nOutputs clients: [[HostFromGpu(GpuCAReduce{add}{1}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cuda error: kernel_reduce_ccontig_node_meb404c8cd39208f6884dd773b584b7d7_0: out of memory. (grid: 1 x 1; block: 256 x 1 x 1)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b8687f325c41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared_randomstreams\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomStreams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_initial_driver_test\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_driver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_nvidia_driver1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m if (config.device.startswith('cuda') or\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/sandbox/cuda/tests/test_driver.py\u001b[0m in \u001b[0;36mtest_nvidia_driver1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m             'but got:']+[str(app) for app in topo])\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         raise Exception(\"The nvidia driver version installed with this OS \"\n\u001b[0;32m     40\u001b[0m                         \u001b[1;34m\"does not give good results for reduction.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cuda error: kernel_reduce_ccontig_node_meb404c8cd39208f6884dd773b584b7d7_0: out of memory. (grid: 1 x 1; block: 256 x 1 x 1)\n\nApply node that caused the error: GpuCAReduce{add}{1}(<CudaNdarrayType(float32, vector)>)\nToposort index: 0\nInputs types: [CudaNdarrayType(float32, vector)]\nInputs shapes: [(10000,)]\nInputs strides: [(1,)]\nInputs values: ['not shown']\nOutputs clients: [[HostFromGpu(GpuCAReduce{add}{1}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin-1')\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cifar():\n",
    "    trn = [unpickle('/home/ubuntu/cifar_10/data_batch_%d' % i) for i in range(1, 6)]\n",
    "    cifar_X_1 = np.concatenate([d['data'] for d in trn]).astype('float32')\n",
    "    cifar_y_1 = np.concatenate([d['labels'] for d in trn]).astype('int32')\n",
    "\n",
    "    tst = unpickle('/home/ubuntu/cifar_10/test_batch')\n",
    "    cifar_X_2 = tst['data'].astype('float32')\n",
    "    cifar_y_2 = np.array(tst['labels'], dtype='int32')\n",
    "\n",
    "    cifar_X = np.r_[cifar_X_1, cifar_X_2]\n",
    "    cifar_y = np.r_[cifar_y_1, cifar_y_2]\n",
    "\n",
    "    cifar_X = cifar_X / 255.\n",
    "\n",
    "    train_X, test_X, train_y, test_y = train_test_split(cifar_X, cifar_y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    return (train_X, test_X, train_y, test_y)\n",
    "\n",
    "\n",
    "def check_homework():\n",
    "    train_X, test_X, train_y, test_y = load_cifar()\n",
    "\n",
    "    # validate for small dataset\n",
    "    train_X_mini = train_X[:1000]\n",
    "    train_y_mini = train_y[:1000]\n",
    "    test_X_mini = test_X[:1000]\n",
    "    test_y_mini = test_y[:1000]\n",
    "\n",
    "    pred_y = homework(train_X_mini, train_y_mini, test_X_mini)\n",
    "    return f1_score(test_y_mini, pred_y, average='macro')\n",
    "\n",
    "if 'homework' in globals():\n",
    "    result = check_homework()\n",
    "\n",
    "    print(\"No Error Occured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.signal import pool\n",
    "#from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#rng = np.random.RandomState(1234)\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='latin-1')\n",
    "    return data\n",
    "\n",
    "trn = [unpickle('/home/ubuntu/cifar_10/data_batch_%d' % i) for i in range(1, 6)]\n",
    "cifar_X_1 = np.concatenate([d['data'] for d in trn]).astype('float32')\n",
    "cifar_y_1 = np.concatenate([d['labels'] for d in trn]).astype('int32')\n",
    "\n",
    "tst = unpickle('/home/ubuntu/cifar_10/test_batch')\n",
    "cifar_X_2 = tst['data'].astype('float32')\n",
    "cifar_y_2 = np.array(tst['labels'], dtype='int32')\n",
    "\n",
    "cifar_X = np.r_[cifar_X_1, cifar_X_2]\n",
    "cifar_y = np.r_[cifar_y_1, cifar_y_2]\n",
    "\n",
    "cifar_X = cifar_X / 255.\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(cifar_X, cifar_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,world!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost: 0.745\n",
      "EPOCH:: 1, Validation cost: 0.667, Validation F1: 0.768\n",
      "Running time is 5 minutes\n",
      "Training cost: 0.590\n",
      "EPOCH:: 2, Validation cost: 0.522, Validation F1: 0.815\n",
      "Running time is 9 minutes\n",
      "Training cost: 0.520\n",
      "EPOCH:: 3, Validation cost: 0.464, Validation F1: 0.841\n",
      "Running time is 13 minutes\n",
      "Training cost: 0.540\n",
      "EPOCH:: 4, Validation cost: 0.407, Validation F1: 0.861\n",
      "Running time is 17 minutes\n",
      "Training cost: 0.167\n",
      "EPOCH:: 5, Validation cost: 0.395, Validation F1: 0.872\n",
      "Running time is 21 minutes\n",
      "Training cost: 0.200\n",
      "EPOCH:: 6, Validation cost: 0.404, Validation F1: 0.876\n",
      "Running time is 24 minutes\n",
      "Training cost: 0.148\n",
      "EPOCH:: 7, Validation cost: 0.450, Validation F1: 0.873\n",
      "Running time is 28 minutes\n",
      "Training cost: 0.180\n",
      "EPOCH:: 8, Validation cost: 0.449, Validation F1: 0.879\n",
      "Running time is 32 minutes\n",
      "Training cost: 0.117\n",
      "EPOCH:: 9, Validation cost: 0.448, Validation F1: 0.885\n",
      "Running time is 36 minutes\n",
      "Training cost: 0.100\n",
      "EPOCH:: 10, Validation cost: 0.471, Validation F1: 0.881\n",
      "Running time is 40 minutes\n",
      "Training cost: 0.153\n",
      "EPOCH:: 11, Validation cost: 0.467, Validation F1: 0.883\n",
      "Running time is 44 minutes\n",
      "Training cost: 0.021\n",
      "EPOCH:: 12, Validation cost: 0.463, Validation F1: 0.886\n",
      "Running time is 47 minutes\n",
      "0.86025 [ 3.  5.  4. ...,  4.  1.  4.] [3 5 4 ..., 4 1 4]\n"
     ]
    }
   ],
   "source": [
    "pred_y = homework(train_X, train_y, test_X)\n",
    "print(sum(pred_y==test_y)/len(test_y),pred_y,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736666666667\n"
     ]
    }
   ],
   "source": [
    "print(sum(pred_y==test_y)/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost: 0.801\n",
      "EPOCH:: 1, Validation cost: 0.658, Validation F1: 0.765\n",
      "Running time is 4 minutes\n",
      "Training cost: 0.617\n",
      "EPOCH:: 2, Validation cost: 0.513, Validation F1: 0.822\n",
      "Running time is 7 minutes\n"
     ]
    }
   ],
   "source": [
    "pred_y = homework(train_X, train_y, test_X)\n",
    "print(sum(test_y==pred_y)/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost: 0.642\n",
      "EPOCH:: 1, Validation cost: 0.652, Validation F1: 0.766\n",
      "Running time is 4 minutes\n",
      "Training cost: 0.485\n",
      "EPOCH:: 2, Validation cost: 0.521, Validation F1: 0.819\n",
      "Running time is 6 minutes\n",
      "Training cost: 0.572\n",
      "EPOCH:: 3, Validation cost: 0.486, Validation F1: 0.833\n",
      "Running time is 9 minutes\n",
      "Training cost: 0.364\n",
      "EPOCH:: 4, Validation cost: 0.461, Validation F1: 0.845\n",
      "Running time is 12 minutes\n",
      "Training cost: 0.229\n",
      "EPOCH:: 5, Validation cost: 0.470, Validation F1: 0.852\n",
      "Running time is 14 minutes\n",
      "Training cost: 0.162\n",
      "EPOCH:: 6, Validation cost: 0.487, Validation F1: 0.853\n",
      "Running time is 17 minutes\n",
      "Training cost: 0.161\n",
      "EPOCH:: 7, Validation cost: 0.566, Validation F1: 0.846\n",
      "Running time is 20 minutes\n",
      "Training cost: 0.199\n",
      "EPOCH:: 8, Validation cost: 0.545, Validation F1: 0.856\n",
      "Running time is 23 minutes\n",
      "Training cost: 0.035\n",
      "EPOCH:: 9, Validation cost: 0.538, Validation F1: 0.861\n",
      "Running time is 25 minutes\n",
      "Training cost: 0.121\n",
      "EPOCH:: 10, Validation cost: 0.576, Validation F1: 0.863\n",
      "Running time is 28 minutes\n",
      "Training cost: 0.059\n",
      "EPOCH:: 11, Validation cost: 0.561, Validation F1: 0.864\n",
      "Running time is 31 minutes\n",
      "Training cost: 0.061\n",
      "EPOCH:: 12, Validation cost: 0.635, Validation F1: 0.855\n",
      "Running time is 33 minutes\n",
      "Training cost: 0.068\n",
      "EPOCH:: 13, Validation cost: 0.595, Validation F1: 0.865\n",
      "Running time is 36 minutes\n",
      "Training cost: 0.173\n",
      "EPOCH:: 14, Validation cost: 0.635, Validation F1: 0.859\n",
      "Running time is 39 minutes\n",
      "Training cost: 0.100\n",
      "EPOCH:: 15, Validation cost: 0.638, Validation F1: 0.860\n",
      "Running time is 41 minutes\n",
      "Training cost: 0.046\n",
      "EPOCH:: 16, Validation cost: 0.641, Validation F1: 0.864\n",
      "Running time is 44 minutes\n",
      "Training cost: 0.005\n",
      "EPOCH:: 17, Validation cost: 0.644, Validation F1: 0.865\n",
      "Running time is 47 minutes\n"
     ]
    }
   ],
   "source": [
    "pred_y = homework(train_X, train_y, test_X)\n",
    "print(sum(pred_y==test_y)/len(test_y),pred_y,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.848833333333 [ 3.  9.  6. ...,  0.  7.  9.] [5 9 6 ..., 0 7 9]\n"
     ]
    }
   ],
   "source": [
    "print(sum(pred_y==test_y)/len(test_y),pred_y,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,world!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost: 0.813\n",
      "EPOCH:: 1, Validation cost: 0.641, Validation F1: 0.780\n",
      "Running time is 8 minutes\n",
      "Training cost: 0.458\n",
      "EPOCH:: 2, Validation cost: 0.502, Validation F1: 0.829\n",
      "Running time is 11 minutes\n",
      "Training cost: 0.331\n",
      "EPOCH:: 3, Validation cost: 0.441, Validation F1: 0.848\n",
      "Running time is 15 minutes\n",
      "Training cost: 0.204\n",
      "EPOCH:: 4, Validation cost: 0.429, Validation F1: 0.860\n",
      "Running time is 19 minutes\n",
      "Training cost: 0.285\n",
      "EPOCH:: 5, Validation cost: 0.403, Validation F1: 0.872\n",
      "Running time is 23 minutes\n",
      "Training cost: 0.150\n",
      "EPOCH:: 6, Validation cost: 0.411, Validation F1: 0.878\n",
      "Running time is 27 minutes\n",
      "Training cost: 0.149\n",
      "EPOCH:: 7, Validation cost: 0.471, Validation F1: 0.860\n",
      "Running time is 31 minutes\n",
      "Training cost: 0.129\n",
      "EPOCH:: 8, Validation cost: 0.435, Validation F1: 0.877\n",
      "Running time is 34 minutes\n",
      "Training cost: 0.096\n",
      "EPOCH:: 9, Validation cost: 0.466, Validation F1: 0.876\n",
      "Running time is 38 minutes\n",
      "Training cost: 0.093\n",
      "EPOCH:: 10, Validation cost: 0.501, Validation F1: 0.873\n",
      "Running time is 42 minutes\n",
      "Training cost: 0.141\n",
      "EPOCH:: 11, Validation cost: 0.442, Validation F1: 0.884\n",
      "Running time is 46 minutes\n",
      "0.860666666667\n"
     ]
    }
   ],
   "source": [
    "pred_y = homework(train_X, train_y, test_X)\n",
    "print(sum(test_y==pred_y)/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,world!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n",
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-3.5.2-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost: 0.651\n",
      "EPOCH:: 1, Validation cost: 0.632, Validation F1: 0.787\n",
      "Running time is 10 minutes\n",
      "Training cost: 0.485\n",
      "EPOCH:: 2, Validation cost: 0.500, Validation F1: 0.823\n",
      "Running time is 15 minutes\n",
      "Training cost: 0.346\n",
      "EPOCH:: 3, Validation cost: 0.415, Validation F1: 0.861\n",
      "Running time is 20 minutes\n",
      "Training cost: 0.409\n",
      "EPOCH:: 4, Validation cost: 0.406, Validation F1: 0.865\n",
      "Running time is 26 minutes\n",
      "Training cost: 0.153\n",
      "EPOCH:: 5, Validation cost: 0.384, Validation F1: 0.875\n",
      "Running time is 31 minutes\n",
      "Training cost: 0.152\n",
      "EPOCH:: 6, Validation cost: 0.391, Validation F1: 0.877\n",
      "Running time is 36 minutes\n",
      "Training cost: 0.122\n",
      "EPOCH:: 7, Validation cost: 0.413, Validation F1: 0.880\n",
      "Running time is 41 minutes\n",
      "Training cost: 0.061\n",
      "EPOCH:: 8, Validation cost: 0.457, Validation F1: 0.876\n",
      "Running time is 46 minutes\n",
      "0.85525\n"
     ]
    }
   ],
   "source": [
    "pred_y = homework(train_X, train_y, test_X)\n",
    "print(sum(pred_y == test_y)/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
